# web-scraper-go
A fast Go-based tool to query SERP API, scrape HTML pages, and prepare them for LLM input.


# web-scraper-go

**web-scraper-go** est un outil ultra-rapide Ã©crit en Go pour interroger Google via SERP API, tÃ©lÃ©charger en parallÃ¨le les pages HTML ciblÃ©es (ex. : `upenn.edu`), les nettoyer efficacement, et les sauvegarder localement. Le pipeline est optimisÃ© pour la performance rÃ©seau, la compression (gzip), et la scalabilitÃ©.

---

## ğŸš€ FonctionnalitÃ©s

- ğŸ” Interroge **SERP API** avec une requÃªte ciblÃ©e (`site:upenn.edu keyword`)
- âš¡ TÃ©lÃ©charge en parallÃ¨le les pages HTML des rÃ©sultats
- ğŸ“¦ Utilise la compression GZIP pour optimiser la bande passante
- ğŸ§¹ Nettoie le HTML pour n'en garder que le texte utile (prÃªt pour LLM)
- ğŸ’¾ Sauvegarde le HTML brut et le texte nettoyÃ© dans `/data`
- ğŸ“Š Affiche des statistiques d'exÃ©cution prÃ©cises pour chaque Ã©tape

---

## ğŸ“¦ Structure du projet

```
web-scraper-go/
â”œâ”€â”€ cmd/                    # Main CLI
â”‚   â””â”€â”€ main.go
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ serpapi/            # RequÃªtes SERP API
â”‚   â”œâ”€â”€ fetcher/            # TÃ©lÃ©chargement & sauvegarde HTML
â”‚   â”œâ”€â”€ freshness/          # (Ã  venir) estimation de fraÃ®cheur
â”‚   â””â”€â”€ llm/                # Nettoyage HTML â†’ texte brut
â”œâ”€â”€ data/                   # Fichiers HTML et/ou texte nettoyÃ©
â”œâ”€â”€ .env.example            # ModÃ¨le de config avec SERP_API_KEY
â”œâ”€â”€ go.mod
```

---

## ğŸ› ï¸ Configuration

1. Clone le repo :
```bash
git clone https://github.com/<your-username>/web-scraper-go.git
cd web-scraper-go
```

2. Installe les dÃ©pendances :
```bash
go mod tidy
```

3. CrÃ©e un fichier `.env` Ã  la racine avec ta clÃ© SERP API :
```
SERP_API_KEY=your_key_here
```

---

## â–¶ï¸ ExÃ©cution

### Construction des requÃªtes

Le flag `--query` permet de spÃ©cifier Ã  la fois le domaine et les mots-clÃ©s de recherche. La syntaxe est :
```
site:domaine.com mot-clÃ©1 mot-clÃ©2 ...
```

Exemples de requÃªtes :
```bash
# Recherche sur upenn.edu
./web-scraper-go --query="site:upenn.edu academic calendar"

# Recherche sur harvard.edu
./web-scraper-go --query="site:harvard.edu admission requirements"

# Recherche sur mit.edu avec plusieurs mots-clÃ©s
./web-scraper-go --query="site:mit.edu computer science graduate program"
```

### Utilisation en ligne de commande

Le programme accepte plusieurs flags pour personnaliser son comportement :

```bash
# Utilisation avec les valeurs par dÃ©faut
./web-scraper-go

# Personnalisation de la requÃªte
./web-scraper-go --query="site:upenn.edu academic calendar"

# Personnalisation complÃ¨te
./web-scraper-go --query="site:upenn.edu academic calendar" --results=5 --output="results"
```

Flags disponibles :
- `--query` : RequÃªte de recherche (dÃ©faut: "site:upenn.edu tuition international students")
  - Format : `site:domaine.com mot-clÃ©1 mot-clÃ©2 ...`
  - Exemple : `site:mit.edu artificial intelligence research`
- `--results` : Nombre de rÃ©sultats Ã  rÃ©cupÃ©rer (dÃ©faut: 3)
- `--output` : Dossier de sortie pour les fichiers (dÃ©faut: "data")

### IntÃ©gration avec Python

Vous pouvez facilement intÃ©grer web-scraper-go dans vos scripts Python :

```python
import subprocess
import json

def scrape_web_pages(domain, keywords, num_results=3, output_dir="data"):
    """
    ExÃ©cute web-scraper-go avec les paramÃ¨tres spÃ©cifiÃ©s.
    
    Args:
        domain (str): Domaine Ã  rechercher (ex: "upenn.edu")
        keywords (str): Mots-clÃ©s de recherche
        num_results (int): Nombre de rÃ©sultats Ã  rÃ©cupÃ©rer
        output_dir (str): Dossier de sortie
    
    Returns:
        dict: RÃ©sultats de l'exÃ©cution
    """
    # Construction de la requÃªte
    query = f"site:{domain} {keywords}"
    
    # Construction de la commande
    cmd = [
        "./web-scraper-go",
        f"--query={query}",
        f"--results={num_results}",
        f"--output={output_dir}"
    ]
    
    # ExÃ©cution de la commande
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True
    )
    
    # VÃ©rification des erreurs
    if result.returncode != 0:
        raise Exception(f"Erreur lors de l'exÃ©cution: {result.stderr}")
    
    return {
        "stdout": result.stdout,
        "stderr": result.stderr,
        "returncode": result.returncode
    }

# Exemple d'utilisation
try:
    results = scrape_web_pages(
        domain="upenn.edu",
        keywords="academic calendar",
        num_results=5,
        output_dir="results"
    )
    print("RÃ©sultats:", results["stdout"])
except Exception as e:
    print(f"Erreur: {e}")
```

Exemple de sortie :
```
URL 1: https://srfs.upenn.edu/...
Temps de fetch: 58 ms
HTML length: 212519 bytes
Texte nettoyÃ© length: 8053 bytes
HTML saved successfully
...
Temps total d'exÃ©cution: 1.8 s
```

---

## ğŸ“ˆ Performances

- Temps total pour 3 pages : **~1.8 secondes**
- Temps moyen de tÃ©lÃ©chargement HTML : **~620 ms/page**
- Traitement full pipeline (fetch + nettoyage + sauvegarde) optimisÃ© Ã  chaque Ã©tape

---

## ğŸ“„ Licence

MIT
